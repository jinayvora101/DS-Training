{"cells":[{"cell_type":"markdown","metadata":{"id":"R0W73NIjLOsl"},"source":["### Gradient Descent Algorithm\n","\n","#### Cost Function\n","It is a function that measures the performance of a model for any given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number.\n","\n","After making a hypothesis with initial parameters, we calculate the Cost function. And with a goal to reduce the cost function, we modify the parameters by using the Gradient descent algorithm over the given data.\n","![alt text](https://images.saymedia-content.com/.image/t_share/MTgxMDExMjk3NjM3NDQyNjQ4/understanding-gradient-descent.jpg)\n","\n","#### Gradient Descent\n","Gradient descent is an iterative optimization algorithm for finding the local minimum of a function.\n","\n","To find the local minimum of a function using gradient descent, we must take steps proportional to the negative of the gradient (move away from the gradient) of the function at the current point. If we take steps proportional to the positive of the gradient (moving towards the gradient), we will approach a local maximum of the function, and the procedure is called Gradient Ascent.\n","\n","Gradient descent was originally proposed by CAUCHY in 1847. It is also known as steepest descent.\n","![alt text](https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png \"Gradient descent\")\n","\n","The goal of the gradient descent algorithm is to minimize the given function (say cost function). To achieve this goal, it performs two steps iteratively:\n","   1. Compute the gradient (slope), the first order derivative of the function at that point\n","   2. Make a step (move) in the direction opposite to the gradient, opposite direction of slope increase from the current point by alpha times the gradient at that point\n","![alt text](https://humanunsupervised.github.io/humanunsupervised.com/topics/images/lesson1/17.png)"]},{"cell_type":"markdown","metadata":{"id":"y_7yLIUSLOso"},"source":["#### Alpha – The Learning Rate\n","Alpha is called Learning rate – a tuning parameter in the optimization process. It decides the length of the steps.\n","\n","We have the direction we want to move in, now we must decide the size of the step we must take. It must be chosen carefully to end up with local minima.\n","\n","If the learning rate is too high, we might OVERSHOOT the minima and keep bouncing, without reaching the minima.\n","If the learning rate is too small, the training might turn out to be too long.\n","![alt text](https://editor.analyticsvidhya.com/uploads/43266images.png \"Alpha\")\n","\n","a) Learning rate is optimal, model converges to the minimum\n","b) Learning rate is too small, it takes more time but converges to the minimum\n","c) Learning rate is higher than the optimal value, it overshoots but converges ( 1/C < η <2/C)\n","d) Learning rate is very large, it overshoots and diverges, moves away from the minima, performance decreases on learning\n","\n","![alt text](https://editor.analyticsvidhya.com/uploads/40982epochss.png \"Learning Rate\")\n","\n","`Note: As the gradient decreases while moving towards the local minima, the size of the step decreases. So, the learning rate (alpha) can be constant over the optimization and need not be varied iteratively.`\n","\n","#### Local Minima\n","The cost function may consist of many minimum points. The gradient may settle on any one of the minima, which depends on the initial point (i.e initial parameters(theta)) and the learning rate. Therefore, the optimization may converge to different points with different starting points and learning rate.\n","\n","![alt text](https://editor.analyticsvidhya.com/uploads/90062gdopt.gif \"Local minima\")\n","\n","The above gif shows convergence of cost function with different starting points"]},{"cell_type":"markdown","metadata":{"id":"Cz4OybZALOsp"},"source":["Gradient descent is simply used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible, so here we will use it to optimize the weight and bias for different regression coefficient."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"eJa-9TlbLOsq"},"outputs":[],"source":["# Supress Warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"U138M5p9LOst","outputId":"c35be0d2-5625-4754-c1d2-a2cfb173f9e5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>TV</th>\n","      <th>radio</th>\n","      <th>newspaper</th>\n","      <th>sales</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>230.1</td>\n","      <td>37.8</td>\n","      <td>69.2</td>\n","      <td>22.1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>44.5</td>\n","      <td>39.3</td>\n","      <td>45.1</td>\n","      <td>10.4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>17.2</td>\n","      <td>45.9</td>\n","      <td>69.3</td>\n","      <td>9.3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>151.5</td>\n","      <td>41.3</td>\n","      <td>58.5</td>\n","      <td>18.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>180.8</td>\n","      <td>10.8</td>\n","      <td>58.4</td>\n","      <td>12.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0     TV  radio  newspaper  sales\n","0           1  230.1   37.8       69.2   22.1\n","1           2   44.5   39.3       45.1   10.4\n","2           3   17.2   45.9       69.3    9.3\n","3           4  151.5   41.3       58.5   18.5\n","4           5  180.8   10.8       58.4   12.9"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["advertising = pd.read_csv(\"advertising.csv\")\n","advertising.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QP_GNFN6LOsv"},"outputs":[],"source":["advertising.drop('Unnamed: 0', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KASnzgooLOsv"},"outputs":[],"source":["x = advertising.drop(['sales'], axis=1)\n","y = advertising['sales']"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"R5uJ7jlWLOsv"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=156)"]},{"cell_type":"markdown","metadata":{"id":"hdcjTM_xLOsx"},"source":["### Stochastic Gradient Descent (SGD) Regressor\n","The word ‘stochastic‘ means a system or a process that is linked with a random probability. Hence, in Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration.\n","\n","In Gradient Descent, there is a term called “batch” which denotes the total number of samples from a dataset that is used for calculating the gradient for each iteration. `In typical Gradient Descent optimization, the batch is taken to be the whole dataset.` Although, using the whole dataset is really useful for getting to the minima in a less noisy and less random manner, but the problem arises when our datasets get big.\n","\n","Suppose, you have a million samples in your dataset, so if you use a typical Gradient Descent optimization technique, you will have to use all of the one million samples for completing one iteration while performing the Gradient Descent, and it has to be done for every iteration until the minima are reached. Hence, it becomes computationally very expensive to perform.\n","This problem is solved by Stochastic Gradient Descent. `SGD uses only a single sample, i.e., a batch size of one, to perform each iteration. The sample is randomly shuffled and selected for performing the iteration.`\n","\n","In SGD, since only one sample from the dataset is chosen at random for each iteration, the path taken by the algorithm to reach the minima is usually noisier than your typical Gradient Descent algorithm. But that doesn’t matter all that much because the path taken by the algorithm does not matter, as long as we reach the minima and with a significantly shorter training time"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"rUOBW_i4LOsx"},"outputs":[],"source":["# Creating model object\n","from sklearn.linear_model import SGDRegressor\n","sgdr = SGDRegressor()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"5oDMDakWLOsx","outputId":"894d2947-94a7-4bf4-e02b-41cd9ead0d26"},"outputs":[{"data":{"text/plain":["SGDRegressor()"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Fitting train data\n","sgdr.fit(xtrain,ytrain)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MVhFm2uzLOsy","outputId":"80bf4061-5a37-4e3d-a615-b79c6c5d11a9"},"outputs":[{"data":{"text/plain":["array([-1.87179512e+10, -3.38693299e+10,  2.00349238e+09])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["sgdr.coef_"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"bexJo4KZLOsy","outputId":"4c0dcdee-19c9-45ba-c149-665880c9aa0b"},"outputs":[{"data":{"text/plain":["array([-2.50015259e+10])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["sgdr.intercept_"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"CBn5RXYOLOsy"},"outputs":[],"source":["# Prediction using sgd\n","sgd_pred = sgdr.predict(xtest)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"cLrG6N07LOsy","outputId":"b4de29f5-369b-4899-fdd8-90b3167b5694"},"outputs":[{"data":{"text/plain":["3125133187207.8643"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Evaluation using mae\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","sgd_mae = mean_absolute_error(ytest,sgd_pred)\n","sgd_mae"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"_HtHUmNwLOsz","outputId":"41841397-2fb0-4a76-d3a3-0ac9923bb84c"},"outputs":[{"data":{"text/plain":["1.2416665550620936e+25"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# MSE\n","sgd_mse = mean_squared_error(ytest,sgd_pred)\n","sgd_mse"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"AfXC8JOQLOsz","outputId":"c677dd88-60b5-46e0-a80a-4b47b4eb22a7"},"outputs":[{"data":{"text/plain":["3523728926949.537"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# RMSE\n","sgd_rmse = mean_squared_error(ytest,sgd_pred,squared=False)\n","sgd_rmse"]},{"cell_type":"markdown","metadata":{"id":"K0Af6AkkLOsz"},"source":["SGD regressor is not giving out predictions accurately for the dataset in question in comparison to linear regression"]},{"cell_type":"markdown","metadata":{"id":"-Yjyvq-FLOsz"},"source":["### end of the notebook."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
